# Neural Network (numpy) 구현해보기

인공신경망의 기본인 neural network를 numpy를 활용하여 구현해보고자 한다.

다음과 같은 인공신경망을 만들고자 한다. 

## L-Layer Neural Network, for binary classification

-이 모델을 기준으로 작성하였다

![image-20210917171920409](2021-09-17-4.numpy_neural_network.assets/image-20210917171920409.png)

# Evn


```python
# imports
import argparse
import os
import random
import shutil
import json
import zipfile
import math
import copy
import collections
import re

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
# import sentencepiece as spm
import tensorflow as tf
import tensorflow.keras.backend as K

from tqdm.notebook import tqdm, trange
```


```python
# 환경 설정
args = {
    # random seed value
    "seed": 1234
}
args = argparse.Namespace(**args)

print(args)
```

    Namespace(seed=1234)



```python
# random seed 설정
random.seed(args.seed)
np.random.seed(args.seed)
tf.random.set_seed(args.seed)
```


```python
# gpu 사용량 확인
!nvidia-smi
```

    NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.



# Sigmoid

먼저 시그모이드 함수부터 정의하겠습니다. 

- x 값을 -10부터 10까지 1000개로 나눈값을 val_x로 정해줍니다.


```python
val_x = np.linspace(-10, 10, 1000)
val_x
```




    array([-10.        ,  -9.97997998,  -9.95995996,  -9.93993994,
            -9.91991992,  -9.8998999 ,  -9.87987988,  -9.85985986,
            -9.83983984,  -9.81981982,  -9.7997998 ,  -9.77977978,
            -9.75975976,  -9.73973974,  -9.71971972,  -9.6996997 ,
            ***(생략)
             9.77977978,   9.7997998 ,   9.81981982,   9.83983984,
             9.85985986,   9.87987988,   9.8998999 ,   9.91991992,
             9.93993994,   9.95995996,   9.97997998,  10.        ])

- sigmoid_y 는 시그모이드 식을 표현한다
- ![Sigmoid 함수 미분 정리](data:image/png;base64,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)


```python
sigmoid_y = 1 / (1 + np.exp(-val_x))
# draw plot
plt.plot(val_x, sigmoid_y)
plt.show()
```

​    

![png](output_7_0.png)
    

- sigmoid 미분한 값

```python
dev_sigmoid_y = 1 / (1 + np.exp(-val_x)) * (1 - 1 / (1 + np.exp(-val_x)))
# draw plot
plt.plot(val_x, dev_sigmoid_y)
plt.show()
```


![png](output_8_0.png)
    

- 함수로 정의해준다.

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```


```python
def sigmoid_deriv(y):
    return y * (1 - y)
```

# Layer1


```python
x = np.array([
    [0, 0, 1],
    [1, 1, 1],
    [1, 0, 1],
    [0, 1, 1],
])  # (bs, 3)
x
```

- x의 벡터값이 다음과 같이 입력된 것을 확인할 수 있다.


    array([[0, 0, 1],
           [1, 1, 1],
           [1, 0, 1],
           [0, 1, 1]])

- y_true 값 , 즉 결과같인 0,1,1,0 으로 입력해서 알려준다.


```python
y_true = np.array([[0, 1, 1, 0]]).T
y_true  # (bs, 1)
```

- 정해진 y 값 (앞으로 학습해서 궁극적인 정답을 찾을 값)


    array([[0],
           [1],
           [1],
           [0]])

- lr 은 learning rate 값으로 학습하면서 정답에 가까워지기위해 변화시킬 크기(변화량)을 의미한다.
- 너무 크게 설정해줄 경우 그 사이의 정답으로 부터 너무 멀어져서 학습의 정확도가 낮아질 수 있으며
- 너무 작을 경우 학습하는데 시간이 너무 오래걸릴 수 있다.


```python
lr = 0.01
```


```python
Wo = np.random.randn(3, 1)  # (3, 1)
bo = np.zeros((1,))  # (1,)

Wo, bo
```

- neural network의 파라미터인 wo, bo를 정해준다.


    (array([[ 0.47143516],
            [-1.19097569],
            [ 1.43270697]]), array([0.]))



## forward

- 먼저 ho = x * wo + b 이고
  - numpy의 벡터 곱셈인 matmul을 활용해서 h0값을 구해준다.


```python
ho = np.matmul(x, Wo) + bo  # (bs, 3), (3, 1), (1,) -> (bs, 1)
ho
```


    array([[1.43270697],
           [0.71316644],
           [1.90414213],
           [0.24173127]])

- sigmoid를 취해줘서 y의 예측값을 구해준다.


```python
y_pred = sigmoid(ho)  # (bs, 1) -> (bs, 1)
y_pred
```




    array([[0.80732274],
           [0.67110045],
           [0.87035962],
           [0.56014025]])

- MSE = square( y - hat y)


```python
MSE = np.square(y_true - y_pred)  # (bs, 1), (bs, 1) -> (bs, 1)
MSE
```




    array([[0.65177001],
           [0.10817491],
           [0.01680663],
           [0.3137571 ]])




```python
np.mean(MSE)
```




    0.27262716361994604

- 확률값으로 0.5	보다 크면 1로 준다(sigmoid 의 경우 확률값이 0~1 사이이므로 로지스틱 분류를 할 때 유용)


```python
y_pred_class = (y_pred > 0.5).astype(np.float)  # (bs, 1) -> (bs, 1)
y_pred_class
```




    array([[1.],
           [1.],
           [1.],
           [1.]])

- 예측한 y 값고 실제 y 값이 일치하는지 확인해준다.


```python
y_match = (y_true == y_pred_class).astype(np.float)
y_match
```




    array([[0.],
           [1.],
           [1.],
           [0.]])

- accuracy를 확인한다.
- 4개 중에 2개 맞고, 2개 틀렸으므로 accuracy가 0.5임


```python
acc = np.sum(y_match) / max(y_true.shape[0], 1)  # (bs, 1) -> (1,)
acc
```




    0.5



## backward

- 결과값을 가지고 오차를 구해서, 오차값을 앞으로 전달해서 weight 값을 수정하는 backward 방법으로 해보자

![image-20210927162757813](2021-09-17-4.numpy_neural_network.assets/image-20210927162757813.png)

- 미분한 MSE 값은 (y_true - y_pred)^1/2를 미분한 것이니까 아래와 같이 미분값이 나온다.


```python
dMSE_dy = -2 * (y_true - y_pred)  # (bs, 1), (bs, 1) -> (bs, 1)
dMSE_dy
```




    array([[ 1.61464548],
           [-0.6577991 ],
           [-0.25928077],
           [ 1.1202805 ]])

- 체인으로 된 미분값을 구하기 위해서 
- dy/dho = deriv(sigmoid(y_pred))
- 


```python
dy_dho = sigmoid_deriv(y_pred)  # (bs, 1) -> (bs, 1)
dy_dho  # (bs, 1)
```




    array([[0.15555273],
           [0.22072464],
           [0.11283376],
           [0.24638315]])




```python
dMSE_dho = dMSE_dy * dy_dho  # (bs, 1), (bs, 1) -> (bs, 1)
dMSE_dho
```




    array([[ 0.25116252],
           [-0.14519247],
           [-0.02925562],
           [ 0.27601824]])




```python
dho_dWo = x  # (bs, 3)
dho_dWo
```




    array([[0, 0, 1],
           [1, 1, 1],
           [1, 0, 1],
           [0, 1, 1]])




```python
dho_dbo = np.array([[1, 1, 1, 1]]).T  # (bs, 1)
dho_dbo
```




    array([[1],
           [1],
           [1],
           [1]])




```python
delta_Wo = np.dot(dho_dWo.T, dMSE_dho)  # (bs, 3).T, (bs, 1) -> (3, 1)
delta_Wo
```




    array([[-0.17444809],
           [ 0.13082577],
           [ 0.35273266]])




```python
delta_bo = np.dot(dho_dbo.T, dMSE_dho)  # (bs, 1).T, (bs, 1) -> (1, 1)
delta_bo
```




    array([[0.35273266]])




```python
Wo -= lr * delta_Wo  # (3, 1), (3, 1) -> (3, 1)
Wo
```




    array([[ 0.47317964],
           [-1.19228395],
           [ 1.42917964]])




```python
bo -= lr * delta_bo.squeeze()  # (1,), (1,) -> (1,)
bo
```




    array([-0.00352733])



## train


```python
lr = 0.01
```


```python
Wo = np.random.randn(3, 1)  # (3, 1)
bo = np.zeros((1,))  # (1,)

Wo, bo
```




    (array([[-2.24268495],
            [ 1.15003572],
            [ 0.99194602]]), array([0.]))



## 실습
- forward & backward를 1000번 반복하면서 학습되는 과정을 확인해 보세요.


```python
losses, acces = [], []
for i in range(1000):
    # forward
    ho = np.dot(x, Wo) + bo  # (bs, 3), (3, 1), (1) -> (bs, 1)
    y_pred = sigmoid(ho)  # (bs, 1) -> (bs, 1)
    # loss
    MSE = np.square(y_true - y_pred)  # (bs, 1), (bs, 1) -> (bs, 1)
    loss = np.mean(MSE)
    losses.append(loss)
    # acc
    y_pred_class = (y_pred > 0.5).astype(np.float)  # (bs, 1) -> (bs, 1)
    y_match = (y_true == y_pred_class).astype(np.float)
    acc = np.sum(y_match) / max(y_true.shape[0], 1)  # (bs, 1) -> (1,)
    acces.append(acc)
    # backward
    dMSE_dy = -2 * (y_true - y_pred)  # (bs, 1), (bs, 1) -> (bs, 1)
    dy_dho = sigmoid_deriv(y_pred)  # (bs, 1) -> (bs, 1)
    dMSE_dho = dMSE_dy * dy_dho  # (bs, 1), (bs, 1) -> (bs, 1)
    dho_dWo = x  # (bs, 3)
    dho_dbo = np.array([[1, 1, 1, 1]]).T  # (bs, 1)
    delta_Wo = np.dot(dho_dWo.T, dMSE_dho)  # (bs, 3).T, (bs, 1) -> (3, 1)
    delta_bo = np.dot(dho_dbo.T, dMSE_dho)  # (bs, 1).T, (bs, 1) -> (1, 1)
    # update weights
    Wo -= lr * delta_Wo  # (3, 1), (3, 1) -> (3, 1)
    bo -= lr * delta_bo.squeeze()  # (1,), (1,) -> (1,)
```


```python
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(losses, 'b-', label='loss')
plt.xlabel('Epoch')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(acces, 'g-', label='acc')
plt.xlabel('Epoch')
plt.legend()

plt.show()
```


​    
![png](output_40_0.png)
​    



```python

```
