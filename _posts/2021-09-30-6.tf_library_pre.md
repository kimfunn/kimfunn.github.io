# 자연어 처리 개발 준비

## 1. 텐서플로

- 텐서플로(TensorFlow) 는 구글에서 2015년에 오픈소스로 발표한 머신러닝 라이브러리이다.
  - - 데이터 플로 그래프를 통한 풍부한 표현력
    - 아이디어 테스트에서 서비스 단계까지 이용가능
    - 계산 구조와 목표 함수만 정의하면 자동으로 미분 계산을 처리
    - 파이썬 /C++ 를 지원하며, SWIG를 통해 다양한 언어지원 가능
    - 유연성과 확장성
- 텐서(Tenser) : N 차원 메트릭스
- 플로(Flow) : 데이터 흐름 그래프를 사용해 수치 연산을 하는 과정
  - 노드(Node) : 수칙연산(operator), 변수(variable),  constant(상수)
  - 엣지(edge): 노드 사이를 이용하는 다차원 데이터 배열(텐서,tensor)

## 2.1.1 tf.keras.layers

텐서플로를 사용한 뎁러닝 모델은  블록을 하나씩 쌓아서 전체 구조를 만들어가는 것 



### 2) tf.keras.layers.Dense

- Dense : 신경망 구조의 가장 기본적인 형태 의미

- 아래의 수식을 만적하는 기본적인 신경망 형태의 층을 만드는 함수


  $$
  y = f(Wx+b)
  $$
  
  
  

### 라이브러리 불러오기 및 상수값 설정


```python
import tensorflow as tf
```


```python
INPUT_SIZE = (20, 1)
CONV_INPUT_SIZE = (1, 28, 28)
IS_TRAINING = True
```

### Dense Layer


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
output = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs) 
```

### Dense Layer with 1 hidden layer

예시 ) 10개의 노드를 가지는 은닉층이 있고 최종 출려 값은 2개의 노드가 있는 신경망 구조


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs)
output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)
```

### Dropout

#### 3) tf.keras.layers.Dropout

신경망 모델을 만들 때 생기는 여러 문제점 중 대표적인 문제점 : 과적함(Overfitting)

Dropout을 활용해서 keras.layers 입력값에 드롭아웃을 적용할 수 있음

- tf.keras.layers.dropout 확률 0.2로 지정 -> 노드의 20%를 0으로 만듦
  - tf.nn.dropout 확률 0.2 로 지정 -> 노드의 (100-20)%를 0으로 만듦


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
dropout = tf.keras.layers.Dropout(rate = 0.5)(inputs)
```

### Dense Layer with 1 hidden layer and dropout


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)
hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(dropout)
output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)
```

### Convolutional layer

- 

- |        |                          |                   |
  | ------ | ------------------------ | ----------------- |
  | Conv1D | 한 방향(가로)            | 1-D Array(vector) |
  | Conv2D | 두 방향(가로,세로)       | 2-D Array(matrix) |
  | Conv3D | 세 방향(가로,세로, 높이) | 3-D Array(tensor) |

  


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
conv = tf.keras.layers.Conv1D(
         filters=10, #필터의 개수, 정수형으로 지정 (출력의 차원 수))
         kernel_size=3, # 필터의 크기, 합성이 적용되는 위도우의 길이
         padding='same',#VALID / SAME
         activation=tf.nn.relu)(inputs)
```

### Convolutional layer with dropout


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
dropout = tf.keras.layers.Dropout(rate=0.2)(inputs)
conv = tf.keras.layers.Conv1D(
         filters=10,
         kernel_size=3,
         padding='same',
         activation=tf.nn.relu)(dropout)

```

### Input -> Dropout -> Convolutional layer -> MaxPooling -> Dense layer with 1 hidden layer -> Output


#### tf.keras.layers.MaxPool1D
- 합성곱과 같이쓰이는 기법
    - 보통 피처 맵(feature map)의 크기를 줄이거나 주요한 특징을 뽑아내기 위해 합성곱 이후 적용되는 기법 ( 맥스풀링 / 평균풀링)
    
- flatten : 풀링을 통해 얻은 행렬을 벡터값으로 변경


```python
inputs = tf.keras.layers.Input(shape = INPUT_SIZE)
dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)
conv = tf.keras.layers.Conv1D(
         filters=10,
         kernel_size=3,
         padding='same',
         activation=tf.nn.relu)(dropout)
max_pool = tf.keras.layers.MaxPool1D(pool_size = 3, padding = 'same')(conv)
flatten = tf.keras.layers.Flatten()(max_pool)
hidden = tf.keras.layers.Dense(units = 50, activation = tf.nn.relu)(flatten)
output = tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)(hidden)  
```


```python

```
